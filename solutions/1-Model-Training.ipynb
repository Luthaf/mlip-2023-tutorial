{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Interatomic Potentials with Metatensor\n",
    "\n",
    "This notebook contains a short example of how one can train a very simple MLIP\n",
    "using metatensor to pass data around. \n",
    "\n",
    "**WARNING**: To ensure a reasonable training time, the dataset size, model\n",
    "complexity and training procedure have been reduced to their absolute minimum.\n",
    "If you want to train a model for your own research, make sure you evaluate all\n",
    "of these and increase them to ensure the resulting MLIP is stable and precise.\n",
    "\n",
    "In general, when training a MLIP we need the following ingredients:\n",
    "\n",
    "- a dataset, containing structures and their energy (and forces/virial). Here we\n",
    "  will use a dataset of conformers of 2-Propen-1-ol, with the energy and forces\n",
    "  computed using [DFTB](https://dftbplus.org/).\n",
    "- a model architecture, which defines how our MLIP makes its predictions. Here\n",
    "  we'll use a perceptron neural network on top of rotation invariants SOAP power\n",
    "  spectrum. This should be very close to the first generation of\n",
    "  Behler-Parrinello NNs.\n",
    "- an optimizer and loss function, used inside a training loop to optimize the NN\n",
    "  weights and ensure the model predictions match the DFTB calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - The Dataset\n",
    "\n",
    "The dateset we will use contains a collection of distorted 2-Propen-1-ol\n",
    "conformations, taken from the ANI-1 dataset (see\n",
    "https://github.com/isayev/ANI1_dataset) with their energies and forces\n",
    "re-computed using DFTB+ (see https://dftbplus.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import ase.io  # read the dataset\n",
    "\n",
    "import chemiscope  # display the structures and associated properties in jupyter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data and extract energies and forces from ASE\n",
    "frames = ase.io.read(\"../propenol_conformers_dftb.xyz\", \":500\")\n",
    "\n",
    "energies = np.array([[f.info[\"energy\"]] for f in frames])\n",
    "forces = np.vstack([f.arrays[\"forces\"] for f in frames])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use chemiscope (https://chemiscope.org/) to visualize the structures and\n",
    "corresponding energies in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemiscope.show(frames, properties={\n",
    "    \"frame_index\": np.arange(len(frames)),\n",
    "    \"energy\": energies,\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - The Model\n",
    "\n",
    "Our model will be built using a basic neural network applied on top of SOAP\n",
    "power spectrum, as computed by rascaline (https://github.com/Luthaf/rascaline/).\n",
    "\n",
    "SOAP (Smooth Overlap of Atomic Position) is a family of atomistic\n",
    "representations, encoding data about a collection of atoms in a way which is\n",
    "well suited for usage with machine learning. In particular, the resulting\n",
    "per-atom SOAP descriptor is invariant to global translations, invariant to\n",
    "permutations of neighbors atoms, and equivariant to rotations. The SOAP power\n",
    "spectrum is the three-body representation, invariant to rotations.\n",
    "\n",
    "SOAP starts by representing atoms with a Gaussian density (instead of point\n",
    "particles), and then expanding the neighbors density around an atom on a set of\n",
    "radial and angular basis. This initial 2-bodies expansion is called the SOAP\n",
    "spherical expansion $\\langle \\alpha n l m | \\rho_i \\rangle$, and there is one\n",
    "such spherical expansion per neighbor species $\\alpha$.\n",
    "\n",
    "$$\n",
    "\\langle \\alpha n l m | \\rho_i \\rangle = \\sum_j \\int R_{nl}(r) \\, Y^l_m(r) \\, \\rho_{ij}^\\alpha(r) \\, dr\n",
    "$$\n",
    "\n",
    "![SOAP density](../images/SOAP.png)\n",
    "\n",
    "From here, the SOAP power spectrum $\\langle \\alpha_1 \\alpha_2 n_1 n_2 l |\n",
    "\\rho_i^2 \\rangle$\n",
    "is taken as correlations of the spherical\n",
    "expansion with itself, building a 3-bodies representation of each atomic\n",
    "environments:\n",
    "\n",
    "$$\n",
    "\\langle \\alpha_1 \\alpha_2 n_1 n_2 l | \\rho_i^2 \\rangle = \\sum_m \\langle \\alpha_1 n_1 l m | \\rho_i \\rangle \\otimes \\langle \\alpha_2 n_2 l m | \\rho_i \\rangle\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(123456)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import rascaline.torch\n",
    "from rascaline.torch import AtomicComposition, SoapPowerSpectrum, systems_to_torch\n",
    "\n",
    "from metatensor.torch import Labels, TensorBlock\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As customary when using PyTorch, our model will be a class inheriting from\n",
    "`torch.nn.Module`. In the `forward` function, we'll take a\n",
    "`rascaline.torch.System`, compute the SOAP power spectrum for all atoms in the\n",
    "system and then send this representation through a neural network. This will\n",
    "give us per-atom energies, that we will then sum together to get the overall\n",
    "prediction.\n",
    "\n",
    "$$\n",
    "E = \\sum_i NN(\\langle \\alpha_1 \\alpha_2 n_1 n_2 l |\n",
    "\\rho_i^2 \\rangle)\n",
    "$$\n",
    "\n",
    "The same NN will be used regardless of the central atom species (this will be a\n",
    "first possible improvement of this model later!).\n",
    "\n",
    "Rascaline outputs the SOAP Power Spectrum in a maximally sparse format, where\n",
    "each central species, species of the first neighbor $\\alpha_1$, and species of\n",
    "the second neighbor $\\alpha_2$ are stored separately, minimizing the memory\n",
    "usage and enabling varied treatments of different blocks. Here, we will just\n",
    "treat the neighbors species as one-hot encodings; and the central species as\n",
    "samples with the same behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SOAPModel(torch.nn.Module):\n",
    "    def __init__(self, soap_parameters, species, energy_offset):\n",
    "        super().__init__()\n",
    "\n",
    "        self.energy_offset = torch.tensor(energy_offset)\n",
    "        self.species = species\n",
    "\n",
    "        self.soap_calculator = SoapPowerSpectrum(**soap_parameters)\n",
    "        self.species_pairs = torch.IntTensor(\n",
    "            [(i, j) for i in species for j in species if i <= j]\n",
    "        )\n",
    "\n",
    "        # Number of features produced by the SOAP calculator,\n",
    "        # i.e. size of the input of the NN\n",
    "        n_soap = (\n",
    "            (soap_parameters[\"max_angular\"] + 1)\n",
    "            * soap_parameters[\"max_radial\"] ** 2\n",
    "            * len(self.species_pairs)\n",
    "        )\n",
    "\n",
    "        # Definition of our NN: one hidden layer,\n",
    "        # SiLU activation, 128-sized latent space\n",
    "        self.soap_nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(\n",
    "                in_features=n_soap, out_features=128, bias=False, dtype=torch.float64\n",
    "            ),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(\n",
    "                in_features=128, out_features=128, bias=False, dtype=torch.float64\n",
    "            ),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(\n",
    "                in_features=128, out_features=1, bias=True, dtype=torch.float64\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        systems: List[rascaline.torch.System],\n",
    "        selected_samples: Optional[Labels] = None,\n",
    "    ):\n",
    "        energies = torch.zeros((len(systems), 1), dtype=torch.float64)\n",
    "        for i, system in enumerate(systems):\n",
    "            soap = self.soap_calculator([system], selected_samples=selected_samples)\n",
    "            soap = soap.keys_to_properties(\n",
    "                Labels([\"species_neighbor_1\", \"species_neighbor_2\"], self.species_pairs)\n",
    "            )\n",
    "            soap = soap.keys_to_samples(\"species_center\")\n",
    "\n",
    "            energies_per_atom = self.soap_nn(soap.block().values)\n",
    "            energies[i] = energies_per_atom.sum()\n",
    "\n",
    "        return energies + self.energy_offset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our model!\n",
    "\n",
    "To simplify the task of the NN, we will enforce a constant energy offset\n",
    "corresponding to some arbitrary energy baseline (here, the mean energy of the\n",
    "training set). \n",
    "\n",
    "| ![TASK](../images/clipboard.png) | Go back to the class definition above, and add the energy offset to the prediction |\n",
    "|-------------------------------|------------------------------------------------------------------------------------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOAP_PARAMETERS = {\n",
    "    \"cutoff\": 3.5,\n",
    "    \"max_radial\": 6,\n",
    "    \"max_angular\": 6,\n",
    "    \"atomic_gaussian_width\": 0.3,\n",
    "    \"center_atom_weight\": 1.0,\n",
    "    \"radial_basis\": {\"Gto\": {}},\n",
    "    \"cutoff_function\": {\"ShiftedCosine\": {\"width\": 0.5}},\n",
    "}\n",
    "\n",
    "energy_offset = energies.mean()\n",
    "model = SOAPModel(\n",
    "    SOAP_PARAMETERS,\n",
    "    species=[1, 6, 8],\n",
    "    energy_offset=energy_offset,\n",
    ")\n",
    "\n",
    "\n",
    "first_energy = model(systems_to_torch(frames[:1]))\n",
    "if torch.abs(first_energy + 290) > 1:\n",
    "    raise Exception(\n",
    "        f\"energy of the first structure is {first_energy.item()}, should be around -290.\"\n",
    "        \"Please modify the class above!\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the tools to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with the inputs (systems) and expected outputs (reference_energies) of our model\n",
    "\n",
    "# systems_to_torch convert anything rascaline accepts as input (which includes\n",
    "# `ase.Atoms`) to a class compatible with Torch and TorchScript\n",
    "systems = systems_to_torch(frames)\n",
    "\n",
    "reference_energies = torch.tensor(energies)\n",
    "\n",
    "# We'll need a loss to compare the prediction to the actual output of the model\n",
    "# let's use the mean square error loss\n",
    "mse_loss = torch.nn.MSELoss()\n",
    "\n",
    "# the optimizer updates the weights of the model according to the gradients\n",
    "# a learning rate of 0.003 allows to learn fast enough while preventing the model\n",
    "# from jumping around in parameter space\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.003)\n",
    "epoch = -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the training! We might have to run the loop multiple times to\n",
    "ensure we reach an OK accuracy. As a starting point, we'll stop with a loss\n",
    "around 0.025, but feel free to come back and try to get the loss even lower!\n",
    "\n",
    "\n",
    "| ![TASK](../images/clipboard.png) | Run the training loop until the loss is below 0.025 |\n",
    "|-------------------------------|-----------------------------------------------------|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = epoch + 1\n",
    "\n",
    "for epoch in range(start, start + 100):\n",
    "    optimizer.zero_grad()  # set all parameters gradients to zero\n",
    "\n",
    "    predicted_energies = model(systems)  # run the model once\n",
    "\n",
    "    loss = mse_loss(predicted_energies, reference_energies)  # compute a loss\n",
    "    print(f\"loss at epoch {epoch} is\", loss.item())\n",
    "\n",
    "    loss.backward()  # backward propagate from the loss, updating all parameters gradients\n",
    "    optimizer.step()  # run one optimizer step, updating the parameters based on gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss.item() > 0.025:\n",
    "    raise Exception(\n",
    "        f\"loss is still too high, please continue running the training loop\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now check the energy prediction we are making against the reference\n",
    "values.\n",
    "\n",
    "In an actual research setting, you would also want to check the predictions your\n",
    "model is making on a validation/hold-out set of structure, to prevent your model\n",
    "from over-fitting your training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_energy = model(systems)\n",
    "\n",
    "plt.scatter(energies, predicted_energy.detach().numpy())\n",
    "\n",
    "x = [np.min(energies), np.max(energies)]\n",
    "plt.plot(x, x, c=\"grey\")\n",
    "\n",
    "plt.title(\"energies\")\n",
    "plt.xlabel(\"reference / eV\")\n",
    "plt.ylabel(\"predicted / eV\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the code in rascaline and metatensor is fully integrated with torch\n",
    "automatic differentiation framework, which allows to compute the gradients of\n",
    "any output with respect to any input. In particular, we can use this to also\n",
    "predict the forces acting on the system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to `rascaline.torch.System`, but now tracking\n",
    "# gradients with respect to positions\n",
    "systems_positions_grad = systems_to_torch(frames, positions_requires_grad=True)\n",
    "\n",
    "# make a new prediction\n",
    "predicted_energy = model(systems_positions_grad)\n",
    "\n",
    "# extract the gradient of the prediction with backward propagation\n",
    "# using `torch.autograd.grad`\n",
    "predicted_forces = torch.autograd.grad(\n",
    "    outputs=predicted_energy,\n",
    "    inputs=[s.positions for s in systems_positions_grad],\n",
    "    grad_outputs=-torch.ones_like(predicted_energy),\n",
    "    create_graph=False,\n",
    "    retain_graph=False,\n",
    ")\n",
    "predicted_forces = torch.vstack(predicted_forces)\n",
    "\n",
    "plt.scatter(forces.flatten(), predicted_forces.detach().numpy().flatten())\n",
    "\n",
    "x = [np.min(forces.flatten()), np.max(forces.flatten())]\n",
    "plt.plot(x, x, c=\"grey\")\n",
    "\n",
    "plt.title(\"forces\")\n",
    "plt.xlabel(\"reference / eV/Å\")\n",
    "plt.ylabel(\"predicted / eV/Å\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the model\n",
    "\n",
    "Now that we have a reasonable model, let's export it! We'll need to define some\n",
    "metadata about out model as well, so the MD engine can know which units\n",
    "conversion to make and what the model can do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from rascaline.torch import metatensor_system_to_rascaline\n",
    "\n",
    "from metatensor.torch.atomistic import MetatensorAtomisticModule\n",
    "from metatensor.torch.atomistic import System as MetatensorSystem\n",
    "from metatensor.torch.atomistic import ModelOutput, ModelCapabilities, ModelRunOptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a class conforming to the `MetatensorAtomisticModule` API. In this\n",
    "API, the model receive as input a single structure and a set of options,\n",
    "including which outputs the engine needs. The model should then return these\n",
    "outputs in a dictionary of `TensorBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExportWrapper(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        # model we are wrapping\n",
    "        self.model = model\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        system: MetatensorSystem,\n",
    "        run_options: ModelRunOptions,\n",
    "    ) -> Dict[str, TensorBlock]:\n",
    "        # check if the energy was even required\n",
    "        if \"energy\" not in run_options.outputs:\n",
    "            return {}\n",
    "\n",
    "        # `run_options.selected_atoms` is used to select a subset of atoms on which the\n",
    "        # model should run. This is used in particular for domain decomposition, where\n",
    "        # the model should only produce an energy for the atoms in the current domain.\n",
    "        selected_atoms = run_options.selected_atoms\n",
    "        if selected_atoms is None:\n",
    "            selected_samples = None\n",
    "        else:\n",
    "            # If the engine selected some atoms, we pass these as\n",
    "            # selected samples to rascaline\n",
    "            selected_samples = Labels(\n",
    "                \"center\", torch.IntTensor(selected_atoms).reshape(-1, 1)\n",
    "            )\n",
    "\n",
    "        # Run the model\n",
    "        energy = self.model(\n",
    "            metatensor_system_to_rascaline(system),\n",
    "            selected_samples=selected_samples,\n",
    "        )\n",
    "\n",
    "        # Return our prediction in a Dict[str, TensorBlock]. Here there is'nt much\n",
    "        # metatdata to attach to the output, but this will change if we are returning\n",
    "        # per-atom energy, or more complex outputs (dipôle moments, electronic density,\n",
    "        # etc.)\n",
    "        return {\n",
    "            \"energy\": TensorBlock(\n",
    "                values=energy.reshape(1, 1),\n",
    "                samples=Labels(\"_\", torch.IntTensor([[0]])),\n",
    "                components=[],\n",
    "                properties=Labels(\"energy\", torch.IntTensor([[0]])),\n",
    "            )\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the model in our export wrapper\n",
    "wrapper = ExportWrapper(model)\n",
    "wrapper = wrapper.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step before exporting our model is to define its capabilities: what\n",
    "can this model compute; what are the expected inputs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our model has a single output: the energy\n",
    "energy_output = ModelOutput(\n",
    "    quantity=\"energy\",\n",
    "    # energy is returned in eV\n",
    "    unit=\"eV\",\n",
    "    # energy is returned globally, not per-atom\n",
    "    per_atom=False,\n",
    ")\n",
    "\n",
    "# overall capabilities of the model\n",
    "capabilities = ModelCapabilities(\n",
    "    # expected unit for the positions and cell vectors\n",
    "    length_unit=\"angstrom\",\n",
    "    # which species can this model work with\n",
    "    species=model.species,\n",
    "    outputs={\n",
    "        \"energy\": energy_output,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can export our model and it's capabilities as a new\n",
    "`MetatensorAtomisticModule`, which will run a couple of checks on the model and\n",
    "handle all the units conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metatensor_model = MetatensorAtomisticModule(wrapper, capabilities)\n",
    "metatensor_model.export(\"propenol-model.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go to the next notebook, and run some Molecular Dynamics with our model!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
